services:

  zookeeper:
    image: bitnami/zookeeper:3.8
    healthcheck:
      test: ["CMD", "zkServer.sh", "status"]
      interval: 5s
      retries: 5
      timeout: 5s
    environment:
      ALLOW_ANONYMOUS_LOGIN: "yes"
    volumes:
      - zookeeper_data:/bitnami/zookeeper

  # --- Three Kafka brokers with SCRAM and ACLs enabled ---
  kafka1:
    image: bitnami/kafka:3.5.1
    hostname: kafka1
    depends_on:
      - zookeeper
    environment:
      - KAFKA_BROKER_ID=1
      - KAFKA_CFG_ZOOKEEPER_CONNECT=zookeeper:2181
      - KAFKA_CFG_LISTENERS=PLAINTEXT://0.0.0.0:9092,SASL_PLAINTEXT://0.0.0.0:9093
      - KAFKA_CFG_ADVERTISED_LISTENERS=PLAINTEXT://kafka1:9092,SASL_PLAINTEXT://kafka1:9093
      - KAFKA_CFG_LISTENER_SECURITY_PROTOCOL_MAP=PLAINTEXT:PLAINTEXT,SASL_PLAINTEXT:SASL_PLAINTEXT
      - KAFKA_CFG_SASL_ENABLED_MECHANISMS=SCRAM-SHA-256
      - KAFKA_CFG_SASL_MECHANISM_INTER_BROKER_PROTOCOL=SCRAM-SHA-256
      - KAFKA_CFG_SECURITY_INTER_BROKER_PROTOCOL=SASL_PLAINTEXT
      - KAFKA_CFG_AUTHORIZER_CLASS_NAME=kafka.security.authorizer.AclAuthorizer
      - KAFKA_CFG_ALLOW_EVERYONE_IF_NO_ACL_FOUND=false
      # Define two SCRAM users: admin (super-user) and client
      - KAFKA_CLIENT_USERS=admin,client
      - KAFKA_CLIENT_PASSWORDS=adminpwd,client-secret
      - KAFKA_SUPER_USERS=User:admin
      - KAFKA_AUTO_CREATE_TOPICS_ENABLE=false
      - KAFA_CREATE_TOPICS=my-topic:3:3
    volumes:
      - kafka1_data:/bitnami/kafka
    healthcheck:
      test: ["CMD", "bash", "-c", "echo>/dev/tcp/localhost/9093"]
      interval: 5s
      timeout: 3s
      retries: 12

  kafka2:
    image: bitnami/kafka:3.5.1
    hostname: kafka2
    depends_on:
      - zookeeper
    environment:
      - KAFKA_BROKER_ID=2
      - KAFKA_CFG_ZOOKEEPER_CONNECT=zookeeper:2181
      - KAFKA_CFG_LISTENERS=PLAINTEXT://0.0.0.0:9092,SASL_PLAINTEXT://0.0.0.0:9093
      - KAFKA_CFG_ADVERTISED_LISTENERS=PLAINTEXT://kafka2:9092,SASL_PLAINTEXT://kafka2:9093
      - KAFKA_CFG_LISTENER_SECURITY_PROTOCOL_MAP=PLAINTEXT:PLAINTEXT,SASL_PLAINTEXT:SASL_PLAINTEXT
      - KAFKA_CFG_SASL_ENABLED_MECHANISMS=SCRAM-SHA-256
      - KAFKA_CFG_SASL_MECHANISM_INTER_BROKER_PROTOCOL=SCRAM-SHA-256
      - KAFKA_CFG_SECURITY_INTER_BROKER_PROTOCOL=SASL_PLAINTEXT
      - KAFKA_CFG_AUTHORIZER_CLASS_NAME=kafka.security.authorizer.AclAuthorizer
      - KAFKA_CFG_ALLOW_EVERYONE_IF_NO_ACL_FOUND=false
      - KAFKA_CLIENT_USERS=admin,client
      - KAFKA_CLIENT_PASSWORDS=adminpwd,client-secret
      - KAFKA_SUPER_USERS=User:admin
    volumes:
      - kafka2_data:/bitnami/kafka
    healthcheck:
      test: ["CMD", "bash", "-c", "echo>/dev/tcp/localhost/9093"]
      interval: 5s
      timeout: 3s
      retries: 12

  kafka3:
    image: bitnami/kafka:3.5.1
    hostname: kafka3
    depends_on:
      - zookeeper
    environment:
      - KAFKA_BROKER_ID=3
      - KAFKA_CFG_ZOOKEEPER_CONNECT=zookeeper:2181
      - KAFKA_CFG_LISTENERS=PLAINTEXT://0.0.0.0:9092,SASL_PLAINTEXT://0.0.0.0:9093
      - KAFKA_CFG_ADVERTISED_LISTENERS=PLAINTEXT://kafka3:9092,SASL_PLAINTEXT://kafka3:9093
      - KAFKA_CFG_LISTENER_SECURITY_PROTOCOL_MAP=PLAINTEXT:PLAINTEXT,SASL_PLAINTEXT:SASL_PLAINTEXT
      - KAFKA_CFG_SASL_ENABLED_MECHANISMS=SCRAM-SHA-256
      - KAFKA_CFG_SASL_MECHANISM_INTER_BROKER_PROTOCOL=SCRAM-SHA-256
      - KAFKA_CFG_SECURITY_INTER_BROKER_PROTOCOL=SASL_PLAINTEXT
      - KAFKA_CFG_AUTHORIZER_CLASS_NAME=kafka.security.authorizer.AclAuthorizer
      - KAFKA_CFG_ALLOW_EVERYONE_IF_NO_ACL_FOUND=false
      - KAFKA_CLIENT_USERS=admin,client
      - KAFKA_CLIENT_PASSWORDS=adminpwd,client-secret
      - KAFKA_SUPER_USERS=User:admin
    volumes:
      - kafka3_data:/bitnami/kafka
    healthcheck:
      test: ["CMD", "bash", "-c", "echo>/dev/tcp/localhost/9093"]
      interval: 5s
      timeout: 3s
      retries: 12

  # --- Logstash reading from Kafka ---
  logstash:
    image: docker.elastic.co/logstash/logstash:8.6.0
    depends_on: [kafka1,kafka2,kafka3]
    volumes:
      - ./config/logstash/pipeline.conf:/usr/share/logstash/pipeline/logstash.conf:ro

  # --- AKHQ UI over SASL + basic-auth ---
  akhq:
    image: tchiotludo/akhq:latest
    depends_on: [kafka1,kafka2,kafka3]
    ports:
      - "8080:8080"
    volumes:
      - ./config/akhq.yml:/app/application.yml:ro

  # --- Python producer ---
  producer:
      image: python:3.9-slim
      depends_on:
        kafka1:
          condition: service_healthy
        kafka2:
          condition: service_healthy
        kafka3:
          condition: service_healthy
      restart: unless-stopped                           # ← will restart on failure
      volumes:
        - ./producer:/app:ro
      working_dir: /app
      environment:
        BOOTSTRAP_SERVERS: kafka1:9093,kafka2:9093,kafka3:9093
        TOPIC: my-topic
        SASL_USERNAME: client
        SASL_PASSWORD: client-secret
      entrypoint: >
        bash -c "
          echo '⏱ Waiting for Kafka to be available…' &&
          # use bash /dev/tcp to poll port
          until echo > /dev/tcp/kafka1/9093; do
            sleep 2
          done &&
          echo '✅ Kafka is up, installing dependencies…' &&
          pip install --no-cache-dir kafka-python &&
          echo '🚀 Starting producer…' &&
          python producer.py"
      
  # --- Python consumer ---
  consumer:
    image: python:3.9-slim
    depends_on:
      kafka1:
        condition: service_healthy
      kafka2:
        condition: service_healthy
      kafka3:
        condition: service_healthy
    restart: unless-stopped
    volumes:
      - ./consumer:/app:ro
    working_dir: /app
    environment:
      BOOTSTRAP_SERVERS: kafka1:9093,kafka2:9093,kafka3:9093
      TOPIC: my-topic
      GROUP_ID: my-group
      SASL_USERNAME: client
      SASL_PASSWORD: client-secret
    entrypoint: >
      bash -c "
        echo '⏱ Waiting for Kafka to be available…' &&
        until echo > /dev/tcp/kafka1/9093; do
          sleep 2
        done &&
        echo '✅ Kafka is up, installing dependencies…' &&
        pip install --no-cache-dir kafka-python &&
        echo '🚀 Starting consumer…' &&
        python consumer.py
      "
volumes:
  zookeeper_data:
  kafka1_data:
  kafka2_data:
  kafka3_data: